{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper review\n",
    "## Distributionally Robust Weighted k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this paper relate to the problem at hand?\n",
    "\n",
    "- The current classification problem have a limited amount of training data per class where we want to develop a machine learning (ML) algorithm that can generalize well to a new data. This can be a challenging task for standard ML linear and tree models and even complex models as deep neural networks. So proposed method in paper could address the problem of learning a few-training-sample per class setting in the training data.\n",
    "\n",
    "- The proposed method is a variant of the k-nearest neighbors (k-NN) algorithm that works by finding the k most similar training samples to a new data point and predicting the class of the new data point by assigning different weights to the k nearest neighbors based on their similarity to the new data point and their class labels. In addition to that the weighting process makes algorithm robust to noise and outliers in the training data.\n",
    "\n",
    "- The paper also shows the results where it outperforms the state of the art methods in a few-training-sample per class setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to research in this direction?\n",
    "\n",
    "There are a number of reasons to research in this area:\n",
    "- Limited amount of labeled data per class.\n",
    "  \n",
    "- Collecting and annotating large amount of data requires a significant amount of resources, time and money. The data labeling process can be outsourced, however it become a challenging process where data might be very sensitive and fall under regulations and compliance of the company.\n",
    "\n",
    "- The labeled data might be noisy, incomplete or biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the strengths and weaknesses of the suggested approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths:\n",
    " - Robust to noise and outliers in labeled data.\n",
    " - Well generalization performance in a few-training-sample setting.\n",
    " - Simple and efficient.\n",
    "\n",
    "Weaknesses:\n",
    "- Computational cost\n",
    "- Sensitivie to hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the suggested approach scale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Distributed approach where the training data is divided into multiple splits and distributed among multiple machines to compute distances between a new data point and the splited training data (subset). In the result the aggregated list of distances is derived from each machines to find the k nearest neighbors to the new data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
